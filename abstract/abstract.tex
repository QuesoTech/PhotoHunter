\documentclass{article}

\usepackage{hyperref, tablefootnote}

\title{PhotoHunter: A Citizen-Scientist Game with User-in-the-loop 
  Data Confirmation for Collecting Computer Vision Datasets of
  Geo-tagged Imagery through Crowd-Sourcing Data Collection; Abstracting
  Research using an Interactive, Game-based Mobile Application for Large
  Dataset Creation 
  \footnote{Project location:
    \url{https://github.com/connorgreenwell/PhotoHunter}
  }
}

\author{Connor Greenwell \and Ryan Baltenberger 
  \and J.\ David Smith \and Aaron Bradshaw
  \and Scott Workman\footnote{Advisor}}

\begin{document}

\maketitle

\section{What is ``Citizen-Science''?}

Citizen-science is a form of crowdsourcing in which participants
collect or analyze data. Typically, the tasks they perform are simple
in nature. For example, they may be asked to take a photo, measure the 
temperature, or translate text.

This model is successful due to the ability of humans to perform such
tasks extremely easily and in large numbers, often at little to no
cost in time or effort. For a single research team to collect such
large amounts of data on their own would be both costly and time
consuming. Often, automating these tasks is impossible for computers,
or is in fact the very aim of a projects principle investigators.

\section{Why a Game?}

One of the drawbacks of citizen-science style is the sharp fall-off in
user contributions as the initial interest wears off. One way to
mitigate this decline in contribution is to ``game-ify'' the process.
By abstracting out the actual data collection and telling users they
are participating in a global scale scavenger hunt, we expect that
users will stay engaged and, more importantly, keep contribuing data
for scientists to study.

Part of ``game-ify-ing'' PhotoHunter will involve assigning point
values for both collecting and reviewing data. The users interaction
with the website should be short, sweet, and rewarding. One of the
ways to reward users is the use of a leaderboard.

\section{What Are ``Computer Vision Datasets''?}

As in other fields related to Artificial Intelligence or Machine
Learning, having data to train or test on is integral to the success
of Computer Vision projects. In Computer Vision, these datasets are 
collections of images or videos, each with
associated metadata. For example, in \cite{islam2014geofaces} they
propose a collection of front-facing facial images each with an
accompanying latitude-longitude pair. 

What \cite{islam2014geofaces} and many others have in common
is the difficulty they experienced when collecting these datasets. In
this document, and the following sections we will propose a system for
reducing these difficulties by proposing a system for crowd-sourcing
images and videos, along with an interface for asserting the validity
of the accompanying metadata.

\section{Saving Money}

Researchers have approached dataset creation and labeling by using 
Amazon Mechanical Turk in the past.  Mechanical Turk is a system where
jobs can be posted for users to complete.  The users are given a small 
amount of compensation for completing these tasks.  The downside to this
for researchers is the requirement to fund the completion of the jobs
that they post.  This costs the researchers money that could be used
for other parts of their project instead of dataset creation.  While jobs
can be posted for less than \$0.10, this can quickly add up when you have
tens and even hundreds of thousands of jobs to fund.

\section{PhotoHunter}

\subsection{Data Submission via Mobile App}

From the mobile app, users will be presented with a list of datapoints
to go collect in their area, and a time limit in which each must be
completed. As each task is completed, they will gain points, prompting
them to continue.

Data collection will only occur within the mobile app so that we may
take advantage of built in sensors (time, gps, heading, etc.).

\subsection{Web Interface for Voting on Submissions}

For numerous reasons, users may unknowingly (or knowingly) submit
incorrect data. For example, a dataset that is seeking pictures of
fire hydrants may accidentally receive images of dogs, or trees. In
this case we will employ users to filter submissions. When an image is
submitted, it will be posted for confirmation by other users. The
interface for this will display the image in question, the
requirements, and a simple ``thumbs-up'' and ``thumbs-down'' option.
Each image will be presented for confirmation to numerous users until
we can be reasonably certain that the datapoint should be trusted.

\subsection{Web Interface for Requesting Datasets}

Research groups may be created on request. 

Research groups will have an interface where they may request
datasets. From this interface groups may define GPS bounding boxes,
time frames, and other metadata they are interested in for a given
dataset.

\appendix
\section{Budget}

\begin{table}[h!]
  \centering
  \begin{tabular}{| l | r |}
    \hline
    Item & Cost \\ \hline \hline
    Domain Name(s) \tablefootnote{Based on prices from \url{namecheap.com}} & \$40 \\ \hline
    Virtual Machine Hosting \tablefootnote{Based on prices from \url{digitalocean.com}} & \$60 \\ \hline
    Google Play Store Publishing Fee \tablefootnote{\url{http://developer.android.com/distribute/googleplay/start.html}} & \$25 \\ \hline
    Apple App Store Publishing Fee \tablefootnote{\url{https://developer.apple.com/programs/ios/}} & \$99 \\

    \hline    
    \hline
    Total & \$??? \\
    \hline
  \end{tabular}
\end{table}

\bibliographystyle{plain}
\bibliography{biblio.bib}

\end{document}
